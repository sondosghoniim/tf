# -*- coding: utf-8 -*-
"""data analysis sondos

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ylaKUk4PivL0mLeW2Yi2kBMGnalI6DGU
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from scipy import stats
import warnings

# 1-loading the data

df = pd.read_csv('fifa_eda_stats.csv')
df.head()

#2-Basic exploration.
df.info()
df.describe()
print(df.columns)

# 3-Univariate Analysis.
#histogram, box plot

plt.figure(figsize=(10, 6))
sns.histplot(df['Age'].dropna(), kde=True)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.show()

 #Creating a histogram to visualize the distribution of the 'Age' feature and checking for any patterns or trends.

plt.figure(figsize=(10, 6))
sns.boxplot(x=df['Age'])
plt.title('Box Plot of Age')
plt.xlabel('Age')
plt.show()
#Creating a box plot to identify any potential outliers in the 'Age' feature.

plt.figure(figsize=(10, 6))
sns.boxplot(x=df['Overall'])
plt.title('Boxplot of Overall Rating')
plt.xlabel('Overall Rating')
plt.show()
# Generating a box plot to observe the distribution and detect outliers in the 'Overall' rating feature.

# 4-multivarient analysis.

correlation_matrix = df.corr(numeric_only=True)
print(correlation_matrix)
#Calculating the correlation matrix to understand the relationships between numeric features.

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()


#The heatmap helps in identifying strong or weak correlations between features.

plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x='Age', y='Overall', hue='Preferred Foot', size='Potential', palette='Set1', sizes=(20, 200), alpha=0.1)
plt.title('Age vs Overall Rating by Preferred Foot and Potential')
plt.show()

# Creating a scatter plot to visualize the relationship between Age and Overall Rating,
# while also considering the player's Preferred Foot and Potential.

plt.figure(figsize=(12, 8))
sns.scatterplot(data=df, x='Wage', y='Value', hue='Overall', palette='coolwarm', size='Age', sizes=(20, 200), alpha=1)
plt.xscale('log')
plt.yscale('log')
plt.title('Wage vs Value by Overall Rating and Age')
plt.show()


# Visualizing the relationship between Wage and Value, with Overall Rating as hue and Age as the size of points.
# Log scaling is applied to better handle large ranges of values.

df['Value'] = df['Value'].replace({'€':'', 'M':'e6', 'K':'e3'}, regex=True).astype(float)
df['Wage'] = df['Wage'].replace({'€':'', 'K':'e3'}, regex=True).astype(float)


plt.figure(figsize=(14, 10))
selected_columns = ['Age', 'Overall', 'Potential', 'Wage', 'Value', 'Stamina', 'Strength']
sns.heatmap(df[selected_columns].corr(), annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix of Selected Features')
plt.show()

#Data Cleaning: Converting Value and Wage columns & Removing symbols and converting monetary values from strings to floats to allow further analysis.
#Visualizing the correlation between selected features after cleaning and transforming the data.

#5-Handling Null & Duplicated values.

df['Age'].fillna(df['Age'].median(), inplace=True)
print(df.columns)
df.isnull().sum()
# Filling missing values in the 'Age' column with the median and checking for any remaining null values.

def remove_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df_cleaned = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df_cleaned


df_no_outliers = remove_outliers_iqr(df, 'Value')

def cap_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[column] = df[column].apply(lambda x: lower_bound if x < lower_bound else upper_bound if x > upper_bound else x)
    return df

df_capped = cap_outliers_iqr(df, 'Value')

# Removing & Capping Outliers using IQR Method

def log_transform(df, column):
    df[column] = np.log1p(df[column])
    return df

df_log_transformed = log_transform(df.copy(), 'Value')
# Applying a log transformation to the 'Value' column to reduce skewness and stabilize variance.

def impute_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    median_value = df[column].median()
    df[column] = df[column].apply(lambda x: median_value if x < lower_bound or x > upper_bound else x)
    return df
df_imputed = impute_outliers_iqr(df.copy(), 'Value')

# Replacing outliers in the 'Value' column with the median value to reduce their impact.

#6-One-Hot Encoding and Feature Scaling




categorical_columns = ['Nationality', 'Club', 'Preferred Foot', 'Position']
df_encoded = pd.get_dummies(df, columns=categorical_columns)

numerical_columns = ['Age', 'Overall', 'Potential', 'Value', 'Wage']
scaler = StandardScaler()
df_encoded[numerical_columns] = scaler.fit_transform(df_encoded[numerical_columns])

df_encoded.head()

# Encoding categorical variables and standardizing numerical features for better model performance.

#7-data splitting


X = df.drop(columns=['Overall'])
y = df['Overall']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# Splitting the data into training and testing sets, separating features (X) and target (y) for model training.


